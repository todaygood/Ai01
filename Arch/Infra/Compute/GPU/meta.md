# Meta 24K H100 GPU集群

https://www.51cto.com/article/783608.html

https://www.usenix.org/conference/fast21/presentation/pan

https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/

https://engineering.fb.com/2023/11/15/networking-traffic/watch-metas-engineers-on-building-network-infrastructure-for-ai/

https://engineering.fb.com/2023/09/07/networking-traffic/chakra-execution-traces-benchmarking-network-performance-optimization/

[Grand Teton](https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/)  


## Network 

构建了一个基于Arista 7800系列交换机配合Wedge400和Minipack2 OCP机架式交换机构建的远程直接内存访问（RDMA）在聚合以太网（RoCE）网络结构方案的集群；另一个集群则采用了NVIDIA Quantum2 InfiniBand网络结构。这两种方案均能连接400 Gbps端点。通过这两个不同类型的互连解决方案，我们能够评估它们在大规模训练中的适用性和可扩展性，从而获得更多的见解，指导未来更大规模集群的设计与构建。经过精心的网络、软件和模型架构协同设计，我们成功地在RoCE和InfiniBand集群上运行大型、面向新一代AI（GenAI）的工作负载（包括在RoCE集群上对我们正在进行的Llama 3模型训练），并且未出现任何网络瓶颈。


## Storage 
 
 随着GenAI训练任务逐渐变得更加多模态，消耗大量图像、视频和文本数据，数据存储的需求快速增长。然而，如何在保证高性能的同时兼顾节能，将所有这些数据存储空间紧凑化的问题依然存在，这使得问题更具挑战性。

我们的存储部署通过由Meta专为Flash介质优化的“Tectonic”分布式存储解决方案支持的用户空间Linux文件系统（FUSE）API来满足AI集群的数据和检查点需求。这一解决方案使数千个GPU能够同步保存和加载检查点（这对任何存储解决方案都是一个挑战），同时也提供了用于数据加载所需的灵活、高吞吐量的EB级存储容量。

此外，我们还与Hammerspace合作共同开发并实施了一种并行网络文件系统（NFS）部署，以满足此AI集群的开发者体验要求。Hammerspace带来诸多优势，其中之一便是能够让工程师在数千个GPU环境中进行交互式调试，代码更改会立即对所有节点可见。当Tectonic分布式存储解决方案与Hammerspace相结合时，能够在不牺牲规模的前提下实现快速迭代速度。

我们GenAI集群中的Tectonic和Hammerspace支持的存储部署均基于YV3 Sierra Point服务器平台，并升级到了市场上最新、最大容量的E1.S SSD。除了更高的SSD容量外，我们还根据每台服务器的吞吐量容量、减少机架数量和相关的电源效率等因素定制了每机架的服务器数量。利用OCP服务器如同乐高积木般的构建模块，我们的存储层能够灵活地按需扩展，适应这个集群以及未来更大规模AI集群的要求，同时在日常基础设施维护操作中保持容错性。


[EDSFF硬件外形尺寸标准演进及趋势](https://www.unionmem.com/news_detail-31-51.html)


## 小集群和大集群 

当我们不断探索AI系统的极限时，检验我们设计扩展能力的最好方式就是实际构建系统、优化它并进行测试（虽然模拟器有所帮助，但其局限性明显）。在这个设计过程中，我们将小集群和大集群的性能进行了对比，找出瓶颈所在。下图显示的是，在大量GPU之间以预期达到峰值性能的消息大小进行通信时，AllGather集体操作性能（以0-100标度标准化带宽表示）。

最初，相比优化后的中小型集群，我们大型集群的开箱即用性能较差且不稳定。为了解决这个问题，我们对内部作业调度器进行了改进，使其具有网络拓扑感知能力，从而降低延迟，减少流入网络高层的流量。同时，我们还优化了网络路由策略，并结合NVIDIA Collective Communications Library (NCCL) 的调整，以实现网络资源的最佳利用。这些举措促使我们的大型集群达到了与小型集群同样出色且预期的性能水平。

在图表中，我们可以观察到小型集群（总体通信带宽和利用率）未经优化就能达到90%以上的水平，而未经优化的大规模集群性能表现非常差，利用率仅在10%至90%之间波动。然而，在我们对整个系统（包括软件、网络等方面）进行优化之后，大型集群的性能恢复到了理想的90%以上范围。

除了针对内部基础设施的软件改动，我们还与编写训练框架和模型的团队紧密合作，以适应我们不断发展的基础设施。例如，NVIDIA H100 GPU开启了使用8位浮点数（FP8）等新型数据类型进行训练的可能性。充分利用大型集群需要投入额外的并行化技术，而新的存储解决方案则为跨数千个rank的高度优化检查点运行提供了机会，使其能在数百毫秒内完成。

我们还认识到调试能力是大规模训练的主要挑战之一。在大规模环境下，识别导致整个训练作业停滞的问题GPU变得极其困难。因此，我们正在开发诸如desync debug（异步调试）或分布式集体飞行记录器之类的工具，以揭示分布式训练的详细信息，更快更容易地识别问题。

最后，我们正持续改进PyTorch这一支撑我们AI工作负载的基础AI框架，使其准备好应对成千甚至数万个GPU的训练需求。我们已经识别出了进程组初始化过程中的多个瓶颈，并将启动时间从有时需要数小时缩短到了几分钟。

## Open 

三、对开放AI创新的承诺
Meta始终坚持在AI软件和硬件方面的开放创新承诺。我们坚信开源硬件和软件始终是帮助行业解决大规模问题的宝贵工具。

如今，作为OCP（开放计算项目）的创始成员，我们继续支持开放硬件创新，将诸如Grand Teton和Open Rack等设计向OCP社区开放。同时，我们仍然是PyTorch的主要贡献者，这是为行业内大部分应用场景提供动力的AI软件框架。

我们同样持续致力于AI研究领域的开放创新。我们已经推出了“开放创新AI研究社区”，这是一个与学术研究者合作的伙伴计划，旨在深化我们对如何负责任地开发和分享AI技术的理解，尤其是关注大型语言模型（LLMs）。

对于Meta来说，采取开放的AI方法并不新鲜。我们还发起了AI联盟，这是一个集结了AI行业领先组织的团体，专注于在开放社区中加速负责任的AI创新。我们的AI努力建立在开放科学和跨协作的理念之上。开放生态带来了透明度、审查机制和信任，促进了AI开发的信任与安全，并引领出人人皆可受益、以安全和责任为核心构建的创新成果。

四、Meta未来AI基础设施展望
这两个AI训练集群设计只是我们更大规模AI未来蓝图的一部分。到2024年底，我们的目标是继续扩大基础设施建设，其中包括350,000个NVIDIA H100 GPU，总计算能力相当于近600,000个H100 GPU。