

## LLaMA

https://mp.weixin.qq.com/s/4wnATYFbld_-9SH4UOdIig

“LLaMA，GPT 都是基础大模型，基于 Transformer 架构训练，使用了不同的方法，有 encode-decode 结构的，也有像 GPT 类 decode only 的。所以从零开始的话，就是要基于像 transformer 这样的架构重新开发，并自己训练，各大厂商自研大模型都是如此的。还有一类就是基于现有的基础大模型，有 Continue Pretrain，finetune 等方法，进行再训练或者微调，基于 LLaMA 这样的开源架构，有很多团队在上面做工作。”

也就是说，基于 LLaMA 训练大模型，是国内大模型创业的主流形态，至于从零开始自研，有专家表示：“国内没有几家创业公司能做到这事儿，成本很高，算法要求很高，数据集要求很高，工作量很大，也很容易出错。”

另有行业内人士对虎嗅说道：“国内真正从零到一研发大模型的可能也就三家——百度、阿里、智谱。”

大模型有三大件：算法、数据和权重。

其中，算法以模型架构为载体呈现，也是零一万物饱受非议的焦点；数据则是大家讨论 AIGC 通常会聊到的数据集，相当于给 AI 提供的学习教材；权重是神经网络的基本概念，代表了两个处理单元之间的连接强度。通俗地来理解，“权重”就像一个员工给另一个员工发消息——有的是普通消息，有的是特别提醒，有的是“Ding”一下。而对于接收消息的员工而言，消息权重越高，影响越大。

这与过往的软件产品截然不同。曾几何时，代码等于一切，对应着大模型概念里的“模型架构”。对于大模型来说，架构只是“三大件”的其中之一。在 Mamba 架构面世未久，Transformer 架构一统江湖的当下，甚至架构的重要性还不如数据和权重。而数据和权重属于工程性问题，对应着模型的训练。有知情人透露，OpenAI 训练 GPT-4 时，可能有 20 个团队同时参与，是相当庞大的工程。

所以，Yi-34B 使用 LLaMA 架构，远远谈不上“套壳”，这是大模型技术本身的特殊性决定的。