# MoE 

MoE（Mixture of Experts）架构的大模型是指一类特殊的深度学习模型设计，这种设计采用了多个专家网络或子模型的集合，并通过某种机制动态地分配输入数据到这些专家网络中进行处理。在自然语言处理（NLP）和其他领域的大规模模型应用中，MoE 架构允许模型更加高效地利用其巨大的参数空间，同时还能保持较高的计算效率。

具体来说，在MoE架构的大模型中：

多个专家：模型包含多个专家网络，每个专家擅长处理某一类或某一方面的任务。

动态路由：输入数据（例如，在NLP中是一个个令牌）经过一个门控机制，这个机制决定了每个令牌应该被哪个专家或哪些专家处理。

门控机制：基于输入特征，模型使用一个门控网络来决定各个专家的权重，即如何混合各个专家的输出。

高效计算：由于不是所有专家都处理所有输入，而是只由相关专家处理部分输入，因此在实际运行时，模型可以根据输入内容的不同而灵活调整计算资源，从而减少了不必要的计算开销。

模型扩展性：MoE架构使得构建和训练具有万亿参数级别的大模型成为可能，比如传闻中的GPT-4就可能采用了类似的结构来组合多个较小的GPT-3级别的模型。

综上所述，MoE架构的大模型通过集成多个专家模型的智慧，实现了对复杂问题更精细化和高效的建模能力，尤其在需要处理多种类型任务或者大量数据的情况下表现优越。


[群魔乱舞：MoE大模型详解](https://www.bilibili.com/read/cv30344324/)


什么是MoE大模型？

MoE，全称为Mixed Expert Models，翻译过来就是混合专家模型。MoE并不是什么最新技术，早在1991年的时候，论文Adaptive Mixture of Local Experts就提出了MoE。

我们知道，模型规模是提升模型性能的关键因素之一，这也是为什么今天的大模型能取得成功。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。

MoE 的一个显著优势是它们能够在远少于 Dense 模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。

MoE基于Transformer架构，主要由两部分组成：

稀疏 MoE 层： 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构。

门控网络或路由: 这个部分用于决定哪些 token 被发送到哪个专家。例如，在下图中，“More”这个 token 可能被发送到第二个专家，而“Parameters”这个 token 被发送到第一个专家。有时，一个 token 甚至可以被发送到多个专家。token 的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。

图1 Google Switch Transformer论文中的MoE结构


总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个路由器（或者叫门控网络）和若干数量的专家。 作者：搞学术的温太医 https://www.bilibili.com/read/cv30344324/ 出处：bilibili



## 总结MoE大模型优点，主要有以下3点：

训练速度更快，效果更好。

相同参数，推理成本低。

扩展性好，允许模型在保持计算成本不变的情况下增加参数数量，这使得它能够扩展到非常大的模型规模，如万亿参数模型。

多任务学习能力：MoE在多任务学习中具备很好的新能（比如Switch Transformer在所有101种语言上都显示出了性能提升，证明了其在多任务学习中的有效性）。


## MoE大模型的缺点，主要有以下4点：

训练稳定性：MoE在训练过程中可能会遇到稳定性问题。

通信成本：在分布式训练环境中，MoE的专家路由机制可能会增加通信成本，尤其是在模型规模较大时。

模型复杂性：MoE的设计相对复杂，可能需要更多的工程努力来实现和优化。

下游任务性能：MoE由于其稀疏性，使得在Fine-tuning过程中容易出现过拟合。 作者：搞学术的温太医 https://www.bilibili.com/read/cv30344324/ 出处：bilibili