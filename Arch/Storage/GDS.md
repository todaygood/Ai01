


https://docs.nvidia.com/gpudirect-storage/getting-started/index.html


https://juicefs.com/en/

# HPC集群

RoceIP: 使用whereabouts开源技术，完成集群内Roce网络IP分配， 防止IP重复

Pod多网卡： cni+sriov

单块Nvme设备理论性能2GB/s, 每台使用2块25GB网卡做主主胚子，tcp理论带宽50Gbps
默认使用TCP协议挂载； 

方案采用2个池子， 普通池子和高性能池； 


高性能池： A800/910B ,100G 网络； 8卡 v100 25G 网络；
普通池：  带vGPU虚拟化

EHPC > EHPc in serverless (平替ECS)

EHPC in serverless : 用于推理资源池；

计算：EHPC产品，训练数据同步；
存储：CPFS 并行文件系统，GDS特性
网络: Roce 高性能网络


阿里DTS 的 “NAS数据传输服务”  解决两个NAS之间的数据同步 

阿里CPFS: https://cn.aliyun.com/product/nas_cpfs?from_alibabacloud=

文件存储CPFS（Cloud Parallel File Storage）是阿里云推出的全托管、可扩展并行文件系统，满足高性能计算场景的需求。CPFS提供了统一的命名空间，支持成百上千的机器同时访问，拥有数十GB的吞吐、数百万的IOPS能力的同时还能保证亚毫秒级的延时。

您在阿里云控制台创建CPFS文件系统，通过标准的POSIX文件协议挂载即可以使用。阿里云CPFS特有的数据流动功能可以实现将对象存储OSS中的数据合并入CPFS，进行统一命名空间的元数据管理。您可以手动或者通过自动Lazy-load能力，将OSS中的数据复制到CPFS中，实现通过POSIX文件接口高速访问OSS中的数据。在保持海量数据在OSS中低成本存储的同时，获得高性能文件访问能力。CPFS文件系统应用于AI训练、自动驾驶、基因计算、影视渲染、石油勘探、气象分析、EDA仿真等场景，适用于高吞吐、高IOPS、海量文件的IO密集型业务。


### 华为NFS+

https://we.yesky.com/blog/301070

下图是AI大模型训练过程中一个训练周期的过程图解。shuffle代表将训练模型的数据集打乱，相当于“洗牌”，可以增强算法的鲁棒性，从而加快模型的训练速度。R1-RN代表对每一个batch size数据集的读取。C1-CN代表对每一个bacth size数据集的训练。黄色的wait_read代表GPU闲置等待时间。


AI大模型训练时，采用数据预读取的方式进行，即边训练边读取，当GPU开始训练C1时，这时候可以预读取R2数据集。若存储性能足够强大，理想情况下每一个数据集的训练可以实现无缝衔接，即黄色的wait_read区域应该是不存在的。但实际情况往往并非如此，GPU会存在等待，以第一次出现的wait_read为例，由于存储对R3数据集的读取速度太慢，以至于GPU早已完成了对R2数据集的训练，但只能等存储读取完数据之后才能进行R3数据集的训练。

同理，强大的存储还能够缩短shuffle和CheckPoint保存的时间。为了应对在大模型训练过程中出现的GPU故障、网络故障、超参设置不合理等问题，需要定时保存CheckPoint，且保存CheckPoint时，GPU是需要停止训练的。CheckPoint是用来记录关键点的文件，类似于存储的“快照”功能，其功能是为了实现“断点续训”。时间就是金钱，GPU等待的每分每秒都是金钱在燃烧。

模型训练过程中存储与计算的交互特点可以总结为：以海量小文件读为主，涉及CheckPoint读写操作。也就是说，所有黄色的wait区域，存储都有大幅优化的空间，而IOPS和带宽成为存储性能的关键。鉴于大模型每分每秒都在烧钱，如果不重视存储，整个训练周期下来，黄色区域浪费掉的计算资源将会非常惊人。

这只是存储关键价值的一个典型场景。那么综合来看，AI大模型究竟需要什么样的存储？

第一，数据共享是多个分布式节点训练场景下的存储首要诉求。随着大模型的参数规模越来越大，往往需要几十上百个节点并发训练，若仍采用本地盘的形式，各节点缓存相同副本导致数据成本较高，且本地盘的可扩展性差，单节点SSD能力存在瓶颈，无法实现数据共享。此时，便对数据共享提出了强烈诉求，能够支持数据的高效流转。

第二，海量数据高并发处理能力是大模型时代存储的核心诉求之一。以GPT-4为例，其原始数据集规模已达PB级。AI大模型需要处理海量小文件训练样本，对应海量的元数据操作，同时也要兼顾大文件处理。服务器客户端与存储节点之间要具备高并发，us级低时延的能力。这些都需要存储具有并发访问的能力。

第三，强大的读写性能。在大模型训练阶段，对训练数据样本存储要读得快，对CheckPoint大文件保存也要写的快，将wait时长无限降低，尽可能减少GPU闲置等待时间，提升模型训练效率。这需要存储在大小文件场景下都能提供高性能。

第四，数据存储的高可靠、高安全要求。行业大模型中的数据属于私域数据，其独有的高安全、高可靠性属性且包含敏感信息，要求要有数据备份、远程复制等。CheckPoint是关键性文件，其保存同样需要高性能存储增加可靠性。保障模型训练的稳定性，就是省钱。

第五，向量数据库的快速检索、低时延要求。向量数据库可以一定程度上避免大模型幻觉，及时更新最新的新闻数据等，加强对私域数据的保护。向量数据库是对共有数据集和行业数据集的向量化，由此生成的数据库，可以部署在推理侧，大大加快模型的推理速度。因此，同样需要高性能存储保存向量数据库，加快检索速度。

根据以上这些核心诉求，什么才是适配AI大模型时代的存储，答案已经非常清晰：高性能高可靠的并行文件存储。

并行文件存储支持使用多个 IO 路径将数据读/写到多个存储设备，同时可横向扩展容纳PB级数据，并支持高带宽，天然适配AI大模型对存储的要求。


### Infiniband 网络存储 

目前焱融科技全闪分布式文件存储一体机已经与多家大模型厂商达成合作，其中，与北京智谱华章科技有限公司（简称“智谱AI”）的合作极具代表性

https://www.163.com/dy/article/IO4E6TF30552YZST.html

[打造千亿级别的AI存储系统](https://aigc.luomor.com/2023/09/16/%E6%89%93%E9%80%A0%E5%8D%83%E4%BA%BF%E6%96%87%E4%BB%B6%E9%87%8F%E7%BA%A7%E7%9A%84%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/)


阿里卓越架构  https://help.aliyun.com/product/2362200.html

百度沧海的文件存储CFS 作为百度智能云提供的分布式文件

### 业界一直缺少国产的、好用的并行文件系统

“在AI、云计算、硅基芯片等前沿技术的蓬勃发展下，超算技术正在发生深刻的变革。从硬件、软件、生态的角度出发，构筑怎样高效可靠的数据基础设施，是业界持续探索的问题。”在近日的一场计算学术年会上，华为数据存储产品线副总裁庞鑫接受第一财经记者专访时表示，2023年是AIGC的元年，AI技术将会越来越深入地应用到高性能计算中。

庞鑫认为，传统超算正在加速与AI深度融合，数据密集化趋势会愈加明显。“HPC与AI均是以数学基础、数理逻辑为核心，在各种算力调度与任务协调中，业界已意识到数据快速流转的重要性。因此，这种全新的科学范式催生了全新的数据范式，高效的信息检索与数据处理是超算应用的根基，数据基础设施的升级将为‘超算大厦’的基础，成为关键制胜点。”

超级计算，又称高性能计算（HPC），指利用并行工作的多台计算机系统（即超级计算机）组成的计算资源，处理极端复杂或数据密集型问题。目前，随着多地超算中心和大型数据中心建设升级项目兴起，超级计算市场迅速增长。根据Synergy Research Group数据，全球超级数据中心数量从2017年的390个增长至2022年二季度的659个，增长近一倍，预计2024年总数将超1000个。

“AI技术的导入可以解决传统高性能计算算不了、算不准、算不动的问题，其本质是数据驱动通过AI算法拟合成一个符合大概率的规律，尽管当前这些规律缺乏可解释性，但反而是科研创新突破的最佳指引。”庞鑫对记者表示，割裂的数据分析机制将成为未来制约数据价值变现的主要瓶颈。因此，围绕数据的高效获取、高效清洗、高效流转以及全流程管理将是科技竞备赛中的制胜点，数据决定AI智能的高度。

庞鑫认为，对于科研人员来说，以数据为中心的“数据密集型超算”趋势来临，只有强大的的数据存储，才能保证支撑超算产业从科学研究走向科学智能时代。首先，应推动超算中心算力和存力的协同发展，需要创新算存协同的架构，以存强算，譬如通过将部分算子向存储下沉，实现业务感知的近数据处理，避免在数据准备上花费大量精力；其次，还应部署全系统数据融合加速技术，比如多协议融合互通，让系统前一阶段的输出直接成为下一阶段的输入，实现数据免搬迁；以及全局数据调度能力，让数据可以跨地域、跨系统域地按需流转、快速归集；此外，数据智能分级也非常关键，通过识别数据访问频次让数据在不同介质间分级流动，这在面对超算日趋庞大的数据体量时成为刚需。

庞鑫在采访中谈到，E级超算（每秒可进行百亿亿次数学运算的超级计算机）的建设已经屡见不鲜，随之而来的能耗、散热的严峻问题已成为行业向绿色低碳演进中的重点。通过加大超算中心高密硬件的部署、尤其是闪存介质的应用，能有效解决这一问题。

“目前中国的数据中心使用半导体介质即SSD闪存盘的比例不到30%，闪存盘在性能、能耗、容量密度等多维度上都较传统机械硬盘具有碾压性的表现。华为正在坚定推行全闪存化的分布式存储，通过采用更高容量SSD大盘、场景化的数据缩减算法，正在加速SSD性价比拐点的到来。”庞鑫说。

“此外，文件系统是数据密集型超算演进的灵魂，业界一直缺少国产的、好用的并行文件系统。因此，华为选择了在此节点进行攻坚。”庞鑫表示，超算产业与AI的深度融合是时代的下一个赛点，在算力、算法之外，用好数据是中国超算跨越式发展的新机遇，产业各方应该共同在应用生态的探索上发力，让中国不仅要成为全球超算系统的领跑者，也能够成为全球超算应用的领跑者。

